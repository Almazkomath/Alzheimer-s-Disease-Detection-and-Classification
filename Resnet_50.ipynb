{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa35628-c49d-4423-84c2-43e9f9290db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ✅ Define `load_png_slice` function before using it\n",
    "def load_png_slice(file_path, target_shape=(128, 128)):\n",
    "    \"\"\"Load a PNG image as a grayscale slice, resize, and normalize it.\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {file_path}\")\n",
    "        \n",
    "        img = cv2.resize(img, target_shape, interpolation=cv2.INTER_LINEAR)  # Resize\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_slices(dataset_root):\n",
    "    \"\"\"\n",
    "    Retrieve PNG slices for MCI and NC subjects from Sliced_X_RD, Sliced_Y_RD, and Sliced_Z_RD folders.\n",
    "    \"\"\"\n",
    "    all_slices = []\n",
    "    all_labels = []\n",
    "    missing_subjects = []\n",
    "\n",
    "    categories = [\"MCI\", \"NC\"]\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(dataset_root, category)\n",
    "        if not os.path.exists(category_path):\n",
    "            print(f\"⚠️ Warning: Category folder '{category_path}' not found.\")\n",
    "            continue\n",
    "\n",
    "        for subject in os.listdir(category_path):\n",
    "            subject_path = os.path.join(category_path, subject)\n",
    "            if not os.path.isdir(subject_path):\n",
    "                continue\n",
    "\n",
    "            # Define paths for required slice folders\n",
    "            axis_paths = {\n",
    "                \"X\": os.path.join(subject_path, \"Sliced_X_RD\"),\n",
    "                \"Y\": os.path.join(subject_path, \"Sliced_Y_RD\"),\n",
    "                \"Z\": os.path.join(subject_path, \"Sliced_Z_RD\"),\n",
    "            }\n",
    "\n",
    "            # Check if all required slice folders exist\n",
    "            if not all(os.path.exists(path) for path in axis_paths.values()):\n",
    "                missing_subjects.append(subject)\n",
    "                continue\n",
    "\n",
    "            # Collect and sort slice files\n",
    "            slice_files = {axis: sorted(\n",
    "                [f for f in os.listdir(axis_paths[axis]) if f.endswith(\".png\")],\n",
    "                key=lambda x: int(''.join(filter(str.isdigit, x)))  # Sort numerically\n",
    "            ) for axis in [\"X\", \"Y\", \"Z\"]}\n",
    "\n",
    "            # Ensure each axis has slices\n",
    "            min_slices = min(len(slice_files[\"X\"]), len(slice_files[\"Y\"]), len(slice_files[\"Z\"]))\n",
    "            if min_slices == 0:\n",
    "                missing_subjects.append(subject)\n",
    "                continue\n",
    "\n",
    "            for i in range(min_slices):\n",
    "                slice_paths = {\n",
    "                    \"X\": os.path.join(axis_paths[\"X\"], slice_files[\"X\"][i]),\n",
    "                    \"Y\": os.path.join(axis_paths[\"Y\"], slice_files[\"Y\"][i]),\n",
    "                    \"Z\": os.path.join(axis_paths[\"Z\"], slice_files[\"Z\"][i])\n",
    "                }\n",
    "\n",
    "                # ✅ Now `load_png_slice` is defined, so this won't cause an error\n",
    "                slices = [load_png_slice(slice_paths[axis]) for axis in [\"X\", \"Y\", \"Z\"]]\n",
    "\n",
    "                if any(s is None for s in slices):\n",
    "                    continue\n",
    "\n",
    "                combined_slices = np.stack(slices, axis=-1)\n",
    "                all_slices.append(combined_slices)\n",
    "                all_labels.append(1 if category == \"MCI\" else 0)\n",
    "\n",
    "    if missing_subjects:\n",
    "        print(f\"⚠️ Subjects with missing slices: {len(missing_subjects)}\")\n",
    "\n",
    "    print(f\"✅ Total Slices Loaded: {len(all_slices)}\")\n",
    "\n",
    "    return np.array(all_slices, dtype=np.float32), np.array(all_labels, dtype=np.int32)\n",
    "\n",
    "# 📌 Load Data\n",
    "dataset_root = r\"C:/Users/Alma/Desktop/Data/sliced_data\"\n",
    "data, labels = get_all_slices(dataset_root)\n",
    "\n",
    "if data.shape[0] == 0:\n",
    "    print(\"❌ No valid slices loaded. Please check your dataset paths!\")\n",
    "else:\n",
    "    print(f\"✅ Loaded {len(labels)} samples with shape {data.shape}\")  # Shape should be (N, 128, 128, 3)\n",
    "    print(f\"✅ Labels Shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a669df-3011-435b-8b25-a0187350259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 🔹 Disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "print(\"✅ Running on CPU only:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# 🔹 Assume 'data' and 'labels' are numpy arrays already loaded:\n",
    "# data.shape => (num_samples, 128, 128, 3)\n",
    "# labels.shape => (num_samples,)\n",
    "\n",
    "# Function to create ResNet50-based model\n",
    "def build_resnet_model(input_shape=(128, 128, 3), num_classes=1):\n",
    "    # 🔹 Input Layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # 🔹 Base ResNet50 (no pretrained weights)\n",
    "    base_model = ResNet50(include_top=False, weights=None, input_tensor=inputs)\n",
    "\n",
    "    # 🔹 Add custom head for binary classification\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# 🔹 K-Fold Cross Validation Settings\n",
    "K = 5\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# StratifiedKFold ensures class distribution consistency\n",
    "kf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "# Collect fold metrics\n",
    "fold_accuracies = []\n",
    "fold_losses = []\n",
    "\n",
    "# 🔹 K-Fold Loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(data, labels), 1):\n",
    "    print(f\"\\n🔹 Fold {fold}/{K} 🔹\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = data[train_index], data[val_index]\n",
    "    y_train, y_val = labels[train_index], labels[val_index]\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    # Dataset optimizations\n",
    "    train_dataset = (train_dataset\n",
    "                     .shuffle(len(X_train))\n",
    "                     .batch(BATCH_SIZE)\n",
    "                     .cache()\n",
    "                     .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    val_dataset = (val_dataset\n",
    "                   .batch(BATCH_SIZE)\n",
    "                   .cache()\n",
    "                   .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    # Build and compile the ResNet50 model\n",
    "    model = build_resnet_model()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on validation data\n",
    "    val_loss, val_accuracy = model.evaluate(val_dataset, verbose=0)\n",
    "    print(f\"✅ Fold {fold} - Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4%}\")\n",
    "\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_accuracies.append(val_accuracy)\n",
    "\n",
    "# 🔹 Average Metrics Across Folds\n",
    "print(\"\\n🔹 Cross Validation Results 🔹\")\n",
    "print(f\"✅ Average Validation Accuracy: {np.mean(fold_accuracies):.4%} (+/- {np.std(fold_accuracies):.4%})\")\n",
    "print(f\"✅ Average Validation Loss: {np.mean(fold_losses):.4f} (+/- {np.std(fold_losses):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce471fd-658c-4fb5-8a12-85e6ffbc3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for training dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ✅ Get Model Predictions for Training Data\n",
    "y_train_pred_prob = model.predict(train_dataset)  # Get predicted probabilities\n",
    "y_train_pred = (y_train_pred_prob > 0.5).astype(\"int32\")  # Convert to binary (0 or 1)\n",
    "\n",
    "# ✅ Convert True Labels to Numpy Array\n",
    "y_train_true = np.concatenate([y for _, y in train_dataset], axis=0)  # Extract true labels\n",
    "\n",
    "# ✅ Compute Confusion Matrix for Training Data\n",
    "cm_train = confusion_matrix(y_train_true, y_train_pred)\n",
    "\n",
    "# ✅ Compute Accuracy from Confusion Matrix\n",
    "accuracy_cm_train = np.trace(cm_train) / np.sum(cm_train)  # (TP + TN) / Total Samples\n",
    "\n",
    "# ✅ Compute Accuracy from Model Evaluation on Training Data\n",
    "loss_train, accuracy_eval_train = model.evaluate(train_dataset, verbose=0)\n",
    "\n",
    "# ✅ Plot Training Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['NC', 'MCI'], yticklabels=['NC', 'MCI'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(f\"Training Confusion Matrix\\nModel Accuracy: {accuracy_eval_train:.2%}\")\n",
    "plt.show()\n",
    "\n",
    "# ✅ Print Accuracy & Classification Report for Training Data\n",
    "print(f\"✅ Training Model Accuracy (from Confusion Matrix): {accuracy_cm_train:.2%}\")\n",
    "print(f\"✅ Training Model Accuracy (from Evaluation): {accuracy_eval_train:.2%}\")\n",
    "print(\"\\nTraining Classification Report:\\n\", classification_report(y_train_true, y_train_pred, target_names=['NC', 'MCI']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3f958-5ff8-47ca-a0b5-a4d95c74e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for testing dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ✅ Get Model Predictions\n",
    "y_pred_prob = model.predict(test_dataset)  # Get predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convert to binary (0 or 1)\n",
    "\n",
    "# ✅ Convert True Labels to Numpy Array\n",
    "y_true = np.concatenate([y for _, y in test_dataset], axis=0)  # Extract true labels\n",
    "\n",
    "# ✅ Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ✅ Compute Accuracy from Confusion Matrix\n",
    "accuracy_cm = np.trace(cm) / np.sum(cm)  # (TP + TN) / Total Samples\n",
    "\n",
    "# ✅ Compute Accuracy from Model Evaluation\n",
    "loss, accuracy_eval = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "# ✅ Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NC', 'MCI'], yticklabels=['NC', 'MCI'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(f\"Confusion Matrix\\nModel Accuracy: {accuracy_eval:.2%}\")\n",
    "plt.show()\n",
    "\n",
    "# ✅ Print Accuracy & Classification Report\n",
    "print(f\"✅ Model Accuracy (from Confusion Matrix): {accuracy_cm:.2%}\")\n",
    "print(f\"✅ Model Accuracy (from Evaluation): {accuracy_eval:.2%}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=['NC', 'MCI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4978fac-b9f6-45f5-9ea7-e92e0e275000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
